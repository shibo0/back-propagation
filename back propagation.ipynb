{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a30e83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch, cost is 18.900302666423663 0.3333333333333333\n",
      "1 epoch, cost is 14.894376684225726 0.5833333333333334\n",
      "2 epoch, cost is 13.464756440354959 0.5833333333333334\n",
      "3 epoch, cost is 12.667654401868209 0.625\n",
      "4 epoch, cost is 12.09255329459098 0.7083333333333334\n",
      "5 epoch, cost is 11.63616621859904 0.6666666666666666\n",
      "6 epoch, cost is 11.250820117053934 0.6666666666666666\n",
      "7 epoch, cost is 10.913944573953858 0.7083333333333334\n",
      "8 epoch, cost is 10.614566522067378 0.7083333333333334\n",
      "9 epoch, cost is 10.345085176677765 0.7083333333333334\n",
      "10 epoch, cost is 10.09883482314352 0.7083333333333334\n",
      "11 epoch, cost is 9.869982639136017 0.7083333333333334\n",
      "12 epoch, cost is 9.65371635344127 0.8333333333333334\n",
      "13 epoch, cost is 9.446317024594274 0.8333333333333334\n",
      "14 epoch, cost is 9.24514349511186 0.8333333333333334\n",
      "15 epoch, cost is 9.048565093351177 0.875\n",
      "16 epoch, cost is 8.855833787879734 0.875\n",
      "17 epoch, cost is 8.66687322034684 0.875\n",
      "18 epoch, cost is 8.48199710871788 0.875\n",
      "19 epoch, cost is 8.301621531842462 0.875\n",
      "20 epoch, cost is 8.126050299950489 0.875\n",
      "21 epoch, cost is 7.955371701146602 0.8333333333333334\n",
      "22 epoch, cost is 7.789447109500696 0.8333333333333334\n",
      "23 epoch, cost is 7.627943795215835 0.8333333333333334\n",
      "24 epoch, cost is 7.4703735361484 0.875\n",
      "25 epoch, cost is 7.316124051238075 0.875\n",
      "26 epoch, cost is 7.164493127232313 0.875\n",
      "27 epoch, cost is 7.014750269911952 0.875\n",
      "28 epoch, cost is 6.866255717503867 0.875\n",
      "29 epoch, cost is 6.7186482190199 0.875\n",
      "30 epoch, cost is 6.572048195246562 0.875\n",
      "31 epoch, cost is 6.427132180865627 0.875\n",
      "32 epoch, cost is 6.284950653304031 0.875\n",
      "33 epoch, cost is 6.146580098560538 0.875\n",
      "34 epoch, cost is 6.012864167612744 0.875\n",
      "35 epoch, cost is 5.884342493235277 0.875\n",
      "36 epoch, cost is 5.7612784882051296 0.875\n",
      "37 epoch, cost is 5.64371772739017 0.875\n",
      "38 epoch, cost is 5.531559031098424 0.9166666666666666\n",
      "39 epoch, cost is 5.424620586094249 0.9166666666666666\n",
      "40 epoch, cost is 5.322686874842005 0.9166666666666666\n",
      "41 epoch, cost is 5.225535084890509 0.9166666666666666\n",
      "42 epoch, cost is 5.132947034525538 0.9166666666666666\n",
      "43 epoch, cost is 5.0447131097147375 0.9166666666666666\n",
      "44 epoch, cost is 4.960632599743859 0.9166666666666666\n",
      "45 epoch, cost is 4.8805127873323215 0.9166666666666666\n",
      "46 epoch, cost is 4.804167856041653 0.9166666666666666\n",
      "47 epoch, cost is 4.7314180023657 0.9166666666666666\n",
      "48 epoch, cost is 4.662088834968852 0.9166666666666666\n",
      "49 epoch, cost is 4.596011027288763 0.9166666666666666\n",
      "50 epoch, cost is 4.5330201585202605 0.9166666666666666\n",
      "51 epoch, cost is 4.4729566802384255 0.9166666666666666\n",
      "52 epoch, cost is 4.415665958407057 0.9166666666666666\n",
      "53 epoch, cost is 4.360998353948076 0.9166666666666666\n",
      "54 epoch, cost is 4.308809316327794 0.9166666666666666\n",
      "55 epoch, cost is 4.258959473183563 0.9166666666666666\n",
      "56 epoch, cost is 4.211314705188841 0.9166666666666666\n",
      "57 epoch, cost is 4.165746199667825 0.9166666666666666\n",
      "58 epoch, cost is 4.122130479424388 0.9166666666666666\n",
      "59 epoch, cost is 4.08034940524774 0.9166666666666666\n",
      "60 epoch, cost is 4.04029015190152 0.9166666666666666\n",
      "61 epoch, cost is 4.001845158309817 0.9166666666666666\n",
      "62 epoch, cost is 3.9649120532711715 0.9166666666666666\n",
      "63 epoch, cost is 3.92939355845735 0.9166666666666666\n",
      "64 epoch, cost is 3.8951973707480976 0.9166666666666666\n",
      "65 epoch, cost is 3.8622360261522335 0.9166666666666666\n",
      "66 epoch, cost is 3.8304267476890663 0.9166666666666666\n",
      "67 epoch, cost is 3.7996912796640356 0.9166666666666666\n",
      "68 epoch, cost is 3.7699557107755024 0.9166666666666666\n",
      "69 epoch, cost is 3.7411502884426917 0.9166666666666666\n",
      "70 epoch, cost is 3.713209226654287 0.9166666666666666\n",
      "71 epoch, cost is 3.6860705095111816 0.9166666666666666\n",
      "72 epoch, cost is 3.659675692484006 0.9166666666666666\n",
      "73 epoch, cost is 3.6339697032351825 0.9166666666666666\n",
      "74 epoch, cost is 3.608900643675213 0.9166666666666666\n",
      "75 epoch, cost is 3.5844195947418585 0.9166666666666666\n",
      "76 epoch, cost is 3.5604804252155526 0.9166666666666666\n",
      "77 epoch, cost is 3.537039605720857 0.9166666666666666\n",
      "78 epoch, cost is 3.5140560289160367 0.9166666666666666\n",
      "79 epoch, cost is 3.4914908367444584 0.9166666666666666\n",
      "80 epoch, cost is 3.4693072555138014 0.9166666666666666\n",
      "81 epoch, cost is 3.4474704394835585 0.9166666666666666\n",
      "82 epoch, cost is 3.4259473235772377 0.9166666666666666\n",
      "83 epoch, cost is 3.4047064857925937 0.9166666666666666\n",
      "84 epoch, cost is 3.383718019859155 0.9166666666666666\n",
      "85 epoch, cost is 3.362953418684482 0.9166666666666666\n",
      "86 epoch, cost is 3.3423854691357278 0.9166666666666666\n",
      "87 epoch, cost is 3.321988158716041 0.9166666666666666\n",
      "88 epoch, cost is 3.3017365947101265 0.9166666666666666\n",
      "89 epoch, cost is 3.2816069363821865 0.9166666666666666\n",
      "90 epoch, cost is 3.2615763408026557 0.9166666666666666\n",
      "91 epoch, cost is 3.2416229228464877 0.9166666666666666\n",
      "92 epoch, cost is 3.221725729832011 0.9166666666666666\n",
      "93 epoch, cost is 3.201864731141422 0.9166666666666666\n",
      "94 epoch, cost is 3.1820208229680134 0.9166666666666666\n",
      "95 epoch, cost is 3.1621758480584417 0.9166666666666666\n",
      "96 epoch, cost is 3.142312629952978 0.9166666666666666\n",
      "97 epoch, cost is 3.122415020770769 0.9166666666666666\n",
      "98 epoch, cost is 3.1024679610494954 0.9166666666666666\n",
      "99 epoch, cost is 3.082457549550227 0.9166666666666666\n",
      "100 epoch, cost is 3.0623711203135495 0.9166666666666666\n",
      "101 epoch, cost is 3.0421973236512265 0.9583333333333334\n",
      "102 epoch, cost is 3.0219262072376445 0.9583333333333334\n",
      "103 epoch, cost is 3.0015492930913528 0.9583333333333334\n",
      "104 epoch, cost is 2.981059646068964 0.9583333333333334\n",
      "105 epoch, cost is 2.960451929577807 0.9583333333333334\n",
      "106 epoch, cost is 2.93972244457314 0.9583333333333334\n",
      "107 epoch, cost is 2.918869148533771 0.9583333333333334\n",
      "108 epoch, cost is 2.8978916519699243 0.9583333333333334\n",
      "109 epoch, cost is 2.8767911910450192 0.9583333333333334\n",
      "110 epoch, cost is 2.8555705760085295 0.9583333333333334\n",
      "111 epoch, cost is 2.8342341162540765 0.9583333333333334\n",
      "112 epoch, cost is 2.812787523856239 0.9583333333333334\n",
      "113 epoch, cost is 2.7912377983360734 0.9583333333333334\n",
      "114 epoch, cost is 2.769593096112681 0.9583333333333334\n",
      "115 epoch, cost is 2.7478625885875783 0.9583333333333334\n",
      "116 epoch, cost is 2.7260563130675375 0.9583333333333334\n",
      "117 epoch, cost is 2.7041850207590463 0.9583333333333334\n",
      "118 epoch, cost is 2.6822600258755416 0.9583333333333334\n",
      "119 epoch, cost is 2.660293059508608 0.9583333333333334\n",
      "120 epoch, cost is 2.638296131361211 0.9583333333333334\n",
      "121 epoch, cost is 2.61628140176904 0.9583333333333334\n",
      "122 epoch, cost is 2.594261065698989 0.9583333333333334\n",
      "123 epoch, cost is 2.5722472496685795 0.9583333333333334\n",
      "124 epoch, cost is 2.5502519218313675 0.9583333333333334\n",
      "125 epoch, cost is 2.5282868148679256 0.9583333333333334\n",
      "126 epoch, cost is 2.506363360842958 0.9583333333333334\n",
      "127 epoch, cost is 2.4844926368545845 0.9583333333333334\n",
      "128 epoch, cost is 2.4626853201140952 0.9583333333333334\n",
      "129 epoch, cost is 2.440951651041567 0.9583333333333334\n",
      "130 epoch, cost is 2.4193014030231628 0.9583333333333334\n",
      "131 epoch, cost is 2.397743857621273 0.9583333333333334\n",
      "132 epoch, cost is 2.376287784229 0.9583333333333334\n",
      "133 epoch, cost is 2.354941423386728 0.9583333333333334\n",
      "134 epoch, cost is 2.333712473205996 0.9583333333333334\n",
      "135 epoch, cost is 2.3126080785548053 0.9583333333333334\n",
      "136 epoch, cost is 2.291634822835469 0.9583333333333334\n",
      "137 epoch, cost is 2.2707987223233084 0.9583333333333334\n",
      "138 epoch, cost is 2.250105223129143 0.9583333333333334\n",
      "139 epoch, cost is 2.229559200902222 0.9583333333333334\n",
      "140 epoch, cost is 2.209164963407046 0.9583333333333334\n",
      "141 epoch, cost is 2.1889262560937914 0.9583333333333334\n",
      "142 epoch, cost is 2.168846270744859 0.9583333333333334\n",
      "143 epoch, cost is 2.148927657226457 0.9583333333333334\n",
      "144 epoch, cost is 2.1291725383111944 0.9583333333333334\n",
      "145 epoch, cost is 2.109582527471107 0.9583333333333334\n",
      "146 epoch, cost is 2.0901587494755653 0.9583333333333334\n",
      "147 epoch, cost is 2.0709018635689556 0.9583333333333334\n",
      "148 epoch, cost is 2.051812088951646 0.9583333333333334\n",
      "149 epoch, cost is 2.03288923224646 0.9583333333333334\n",
      "150 epoch, cost is 2.014132716602686 0.9583333333333334\n",
      "151 epoch, cost is 1.995541612070563 0.9583333333333334\n",
      "152 epoch, cost is 1.9771146668711956 0.9583333333333334\n",
      "153 epoch, cost is 1.9588503391888266 0.9583333333333334\n",
      "154 epoch, cost is 1.940746829123562 0.9583333333333334\n",
      "155 epoch, cost is 1.9228021104614146 0.9583333333333334\n",
      "156 epoch, cost is 1.9050139619438191 0.9583333333333334\n",
      "157 epoch, cost is 1.887379997748838 0.9583333333333334\n",
      "158 epoch, cost is 1.869897696930017 0.9583333333333334\n",
      "159 epoch, cost is 1.8525644315948242 0.9583333333333334\n",
      "160 epoch, cost is 1.835377493641524 0.9583333333333334\n",
      "161 epoch, cost is 1.8183341199103769 0.9583333333333334\n",
      "162 epoch, cost is 1.8014315156410496 0.9583333333333334\n",
      "163 epoch, cost is 1.7846668761623636 0.9583333333333334\n",
      "164 epoch, cost is 1.7680374067724973 0.9583333333333334\n",
      "165 epoch, cost is 1.7515403407967556 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 epoch, cost is 1.7351729558359268 1.0\n",
      "167 epoch, cost is 1.718932588240661 1.0\n",
      "168 epoch, cost is 1.7028166458662732 1.0\n",
      "169 epoch, cost is 1.6868226191777798 1.0\n",
      "170 epoch, cost is 1.6709480907869656 1.0\n",
      "171 epoch, cost is 1.6551907435121622 1.0\n",
      "172 epoch, cost is 1.6395483670571154 1.0\n",
      "173 epoch, cost is 1.624018863408509 1.0\n",
      "174 epoch, cost is 1.6086002510524409 1.0\n",
      "175 epoch, cost is 1.5932906681088055 1.0\n",
      "176 epoch, cost is 1.578088374479641 1.0\n",
      "177 epoch, cost is 1.5629917531031037 1.0\n",
      "178 epoch, cost is 1.5479993103994907 1.0\n",
      "179 epoch, cost is 1.533109675989648 1.0\n",
      "180 epoch, cost is 1.518321601759723 1.0\n",
      "181 epoch, cost is 1.5036339603395723 1.0\n",
      "182 epoch, cost is 1.4890457430556654 1.0\n",
      "183 epoch, cost is 1.4745560574129437 1.0\n",
      "184 epoch, cost is 1.460164124154209 1.0\n",
      "185 epoch, cost is 1.4458692739402126 1.0\n",
      "186 epoch, cost is 1.431670943688697 1.0\n",
      "187 epoch, cost is 1.4175686726064696 1.0\n",
      "188 epoch, cost is 1.403562097944987 1.0\n",
      "189 epoch, cost is 1.3896509505069912 1.0\n",
      "190 epoch, cost is 1.3758350499294452 1.0\n",
      "191 epoch, cost is 1.3621142997662767 1.0\n",
      "192 epoch, cost is 1.3484886823932685 1.0\n",
      "193 epoch, cost is 1.334958253756729 1.0\n",
      "194 epoch, cost is 1.3215231379872527 1.0\n",
      "195 epoch, cost is 1.3081835218999045 1.0\n",
      "196 epoch, cost is 1.2949396494024745 1.0\n",
      "197 epoch, cost is 1.2817918158338402 1.0\n",
      "198 epoch, cost is 1.2687403622551132 1.0\n",
      "199 epoch, cost is 1.2557856697168013 1.0\n",
      "[array([[-1.13786836,  0.2729467 , -0.31334177, -0.8535319 , -1.33212833,\n",
      "         1.27032319, -2.0706884 , -0.28971596,  2.14112431, -0.411445  ,\n",
      "        -1.6089566 , -0.11841852,  1.64659512, -0.23791943, -0.63151559,\n",
      "        -0.57451143],\n",
      "       [ 2.27004207,  1.68063929,  1.22278605,  0.2893907 ,  0.60978332,\n",
      "         2.14918063, -0.70306924,  0.6587562 , -1.21350217, -0.91408297,\n",
      "         1.1194655 , -1.06013973, -0.55086405, -0.94407407, -0.39913282,\n",
      "        -2.37127279],\n",
      "       [-1.1121278 , -1.07141773,  1.1102659 , -0.73925872, -0.06679464,\n",
      "         0.59073034, -0.81968342,  0.73509858, -0.88811486, -1.87466111,\n",
      "        -0.76657461,  0.03865093, -0.00390579, -0.18600141,  1.96039777,\n",
      "         0.99622619]]), array([[ 1.13064336e+00,  8.54549521e-01,  3.37346552e-02,\n",
      "        -1.15733223e-01,  4.91892734e-01, -2.38512969e-01,\n",
      "        -3.82721924e-01, -6.09060845e-03,  7.59838615e-01,\n",
      "         5.61088195e-01,  4.26151907e-01,  1.01940669e+00,\n",
      "        -2.74819810e-01, -7.63413968e-02,  1.96510802e-01,\n",
      "        -4.35909998e-01],\n",
      "       [ 9.96933630e-01,  2.96483989e-02,  2.60657775e-01,\n",
      "         6.12901157e-03, -4.87420578e-01,  2.44662558e-01,\n",
      "         3.02924807e-01, -7.77618897e-01,  5.29624405e-01,\n",
      "        -3.72897301e-01, -8.12212290e-01,  2.23577214e-01,\n",
      "        -2.69679442e-01,  5.48751575e-01, -3.13688794e-01,\n",
      "        -7.30977986e-01],\n",
      "       [ 1.55595003e+00, -2.82112580e-01,  3.07459456e-01,\n",
      "         6.12497092e-01, -1.13543349e-01, -1.12917943e+00,\n",
      "        -3.70502332e-01, -8.59934871e-01,  1.00173367e+00,\n",
      "        -2.83579033e-02,  2.85195975e-01, -6.75257268e-01,\n",
      "         1.09658001e-01,  5.88792382e-01, -1.27918479e-01,\n",
      "         3.10377097e-01],\n",
      "       [-6.48429734e-01, -5.58576409e-01,  8.57733404e-01,\n",
      "         1.46616903e-01,  2.61932382e-01, -6.50279398e-01,\n",
      "         2.04331393e-01,  8.03297285e-01, -7.06654549e-02,\n",
      "         9.97221601e-02, -1.19136937e-01, -3.35646418e-01,\n",
      "         2.34071953e-01, -6.17009936e-01, -9.36640754e-02,\n",
      "         5.51250326e-01],\n",
      "       [-3.14320286e-01,  9.59376652e-02,  9.60260278e-01,\n",
      "         4.26807542e-01, -4.13201443e-01, -1.64143553e-01,\n",
      "         5.91368140e-01, -9.66790023e-01, -2.60306409e-01,\n",
      "         6.52915211e-03, -1.92631403e-01,  7.08904291e-01,\n",
      "         8.05251790e-01,  3.92012216e-01,  3.66786322e-01,\n",
      "         2.22272087e-01],\n",
      "       [ 1.05038566e-02, -4.21581362e-01, -1.88062276e-01,\n",
      "        -2.61409256e-01, -3.35216799e-01, -2.01202327e-01,\n",
      "         3.37501325e-01,  4.24628243e-02, -4.38642347e-01,\n",
      "         1.69229825e+00,  8.48919339e-01, -2.52888941e-01,\n",
      "        -6.46269140e-01,  1.01986276e-01,  4.90166662e-01,\n",
      "         4.65374633e-02],\n",
      "       [ 2.87830184e-01, -3.71362723e-01,  4.27326577e-02,\n",
      "         7.18031408e-01,  1.42343154e-01,  4.38452720e-02,\n",
      "        -1.21221843e-01,  3.31527383e-02, -1.04997546e+00,\n",
      "        -1.17996084e+00, -7.02173886e-01, -1.05768595e+00,\n",
      "         6.00667284e-01,  3.63977352e-01,  2.69977208e-01,\n",
      "         5.34883833e-01],\n",
      "       [ 8.11378781e-01, -1.11637862e-01,  6.89507378e-03,\n",
      "         4.56856439e-01,  3.02238440e-01, -6.97077580e-01,\n",
      "         6.14235838e-01, -8.50554808e-01,  1.02245685e-01,\n",
      "        -2.69641503e-02, -5.30709173e-01, -1.24769408e-01,\n",
      "        -1.32883564e-01,  3.69383424e-01,  1.86944530e-01,\n",
      "         4.05525094e-01],\n",
      "       [ 3.84753578e-01, -4.09269122e-01, -3.98096833e-01,\n",
      "        -4.11078171e-01, -1.47713936e-02,  1.00950745e+00,\n",
      "         7.40333447e-01, -1.89846868e-01,  2.94777547e-01,\n",
      "        -1.26464891e+00,  2.05960228e-01, -1.04075265e+00,\n",
      "        -5.48262007e-01, -3.90923871e-01, -1.85348941e-01,\n",
      "        -1.35103971e-02],\n",
      "       [-2.13886247e-01, -2.17428280e-02,  5.33847161e-01,\n",
      "        -1.36409215e+00, -1.32528414e-01,  2.23296950e-01,\n",
      "        -1.22542183e-01, -1.34590780e-01,  1.64274090e-01,\n",
      "        -6.09449458e-01,  9.90508386e-01,  7.83533139e-01,\n",
      "        -8.42358955e-02,  1.33668848e-01, -4.11359144e-02,\n",
      "        -3.95396277e-01],\n",
      "       [-3.43259213e-01,  1.57795651e-01,  1.06038897e+00,\n",
      "         5.69656371e-01, -1.30118586e-01, -4.47085693e-01,\n",
      "        -5.56662191e-01,  5.91336701e-01,  3.16842556e-01,\n",
      "         3.97222731e-01, -4.59516718e-01,  8.64735522e-01,\n",
      "        -3.34404670e-01, -3.24511698e-01,  5.80364918e-02,\n",
      "        -3.73385400e-02],\n",
      "       [ 3.56409122e-01, -3.26043830e-01, -5.54696681e-01,\n",
      "        -5.04027329e-01, -4.72580759e-01, -9.43740518e-02,\n",
      "        -7.50207877e-01,  4.14119767e-01,  1.22185601e-01,\n",
      "        -3.55677072e-01,  1.55361412e-01,  3.04055879e-02,\n",
      "         4.45725696e-01, -1.54100048e-01,  2.08297613e-01,\n",
      "         9.32714645e-02],\n",
      "       [-1.68283869e-01,  2.08665513e-01, -4.34574849e-01,\n",
      "        -7.40576842e-01,  7.84641649e-02,  2.84340403e-04,\n",
      "         3.96372043e-01, -3.55466893e-01, -4.19916111e-01,\n",
      "         9.40228449e-02,  9.69026351e-01,  5.43237031e-02,\n",
      "        -5.63340731e-01, -1.29845104e+00,  2.35184022e-01,\n",
      "        -2.16885599e-01],\n",
      "       [ 1.06273302e+00,  1.59730211e-01, -7.25438936e-02,\n",
      "         1.08059840e+00, -8.79709174e-02, -2.60042222e-01,\n",
      "        -2.30815977e-01, -4.49966548e-01,  1.01554289e+00,\n",
      "        -8.58275157e-01,  1.75991336e-01,  3.23420819e-01,\n",
      "         3.94915545e-01,  4.01596332e-01, -4.26562837e-01,\n",
      "        -3.84881216e-01],\n",
      "       [-3.89021927e-02, -2.85468969e-01, -4.78117511e-01,\n",
      "        -1.70441171e-01,  6.04844176e-01, -6.40789607e-01,\n",
      "        -8.15814600e-02,  2.53617951e-01, -9.59902899e-01,\n",
      "        -3.02166465e-01, -6.71169120e-01, -8.29135051e-01,\n",
      "         2.12356515e-01, -5.12679937e-01, -5.95042466e-01,\n",
      "         6.79092415e-01],\n",
      "       [-1.13927023e+00,  4.46154036e-01, -6.45637838e-01,\n",
      "         2.09855537e-01,  3.34147174e-01, -4.33571928e-01,\n",
      "        -8.32590772e-01,  7.18695988e-01,  7.46705572e-01,\n",
      "         8.50101998e-02,  1.45915805e-01,  7.33330941e-01,\n",
      "         3.41272078e-01, -9.48830277e-01, -1.71155104e-01,\n",
      "         3.09035627e-01]]), array([[-1.91343067,  1.11783538,  0.79477616],\n",
      "       [ 0.39049573,  0.41278063, -0.58789127],\n",
      "       [ 0.32798241,  1.05843559, -1.21200143],\n",
      "       [ 0.89530238,  0.99447782, -0.8095415 ],\n",
      "       [-0.08508942, -0.25636878,  0.03965665],\n",
      "       [ 0.48988645, -1.09477155,  0.75149005],\n",
      "       [-1.17514319, -1.40462212,  0.64018395],\n",
      "       [ 1.10053214, -0.50120453, -1.00680254],\n",
      "       [-0.63850171,  1.57905613, -0.58346169],\n",
      "       [ 1.7497497 , -1.84935166,  0.21854141],\n",
      "       [-0.04295147, -0.39328098,  1.05958649],\n",
      "       [ 1.15441004,  1.2344286 , -1.66485441],\n",
      "       [ 1.16050049, -0.35413292,  0.44555731],\n",
      "       [-1.39681905, -1.0634111 ,  1.05828256],\n",
      "       [ 0.03073708, -0.2895974 , -0.21135439],\n",
      "       [ 0.77137295, -0.42678932,  0.40658233]])]\n",
      "Cost: 0.7852002298870596 Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "np.random.seed(123)  # 需要固定种子，不然初始化参数会变动，训练结果也会变动\n",
    "\n",
    "\n",
    "def parameter_initializer(modelconfig = [3, 16, 16, 3], bias = True):    # n1, n2, n3\n",
    "    \"\"\" 初始化权值 \"\"\"\n",
    "    W = []\n",
    "    for i in range(len(modelconfig)-2):\n",
    "        if bias:\n",
    "            w = np.random.normal(scale=(2.0/modelconfig[i])**0.5, size=(modelconfig[i] + 1, modelconfig[i+1]+1))\n",
    "        else:\n",
    "            w = np.random.normal(scale=(2.0/modelconfig[i])**0.5, size=(modelconfig[i], modelconfig[i+1]))\n",
    "        W.append(w)\n",
    "        \n",
    "    if bias:\n",
    "        w = np.random.normal(scale=(2.0/modelconfig[i])**0.5, size=(modelconfig[-2] + 1, modelconfig[-1]))\n",
    "    else:\n",
    "        w = np.random.normal(scale=(2.0/modelconfig[i])**0.5, size=(modelconfig[-2], modelconfig[-1]))\n",
    "\n",
    "    W.append(w)\n",
    "    return W\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    # sigmoid的导数\n",
    "    return x * (1 - x)\n",
    "\n",
    "def tanh(z):\n",
    "    a = np.tanh(z)\n",
    "    return a\n",
    "\n",
    "def tanh_deriv(z):\n",
    "    z = 1 - z ** 2\n",
    "    return z\n",
    "\n",
    "def forward_propagate(x, W):\n",
    "    \"\"\" 前向传播 \"\"\"\n",
    "\n",
    "    A = []\n",
    "    z = copy.deepcopy(x)\n",
    "        \n",
    "    for i in range(len(W)-1):\n",
    "        z = np.dot(z, W[i])\n",
    "        z = tanh(z)\n",
    "        A.append(z)\n",
    "    \n",
    "    i += 1\n",
    "    z = np.dot(z, W[i])\n",
    "    z = sigmoid(z)\n",
    "    A.append(z)\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "def loss(output, y):\n",
    "    \"\"\" 损失函数 \"\"\"\n",
    "    #print(output.shape, y.shape)\n",
    "    #cross_entropy = -((1-y)*np.log(1-output) + y * np.log(output)) 可以尝试交叉熵\n",
    "    Loss = (output - y) ** 2\n",
    "    cost = np.mean(np.sum(Loss, axis=1))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "\n",
    "def back_propagate(x, W, A, label, config):\n",
    "    A.insert(0, x)\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    D = [(A[-1] - label) * sigmoid_deriv(A[-1])]\n",
    "    n = 0\n",
    "    \n",
    "    for layer in range(len(A)-2, 0, -1):\n",
    "        delta = D[-1].dot(W[layer].T)\n",
    "        delta = delta * tanh_deriv(A[layer])\n",
    "        D.append(delta)\n",
    "    \n",
    "    D = D[::-1]\n",
    "    for layer in range(0, len(W)):\n",
    "        W[layer] += - config['learning_rate'] * A[layer].T.dot(D[layer])\n",
    "    \n",
    "    return W\n",
    "\n",
    "\n",
    "def bpnn_batch(x, y, config):\n",
    "    # 模型参数初始化\n",
    "    W = parameter_initializer(modelconfig = config['model_config'], bias = config['bias'])\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # 批量处理\n",
    "        A = forward_propagate(x, W)\n",
    "        cost = loss(A[-1], y)\n",
    "        W = back_propagate(x, W, A, y, config)\n",
    "        \n",
    "        y_pred = np.argmax(A[-1],axis=1)\n",
    "        y_true = np.argmax(y, axis=1)        \n",
    "        print(\"{} epoch, cost is {}\".format(epoch, cost), sum(list(y_pred==y_true))/y.shape[0])\n",
    "        \n",
    "    return W\n",
    "\n",
    "def bpnn_single(x, y, config):\n",
    "    # 模型参数初始化\n",
    "    W = parameter_initializer(modelconfig = config['model_config'], bias = config['bias'])\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        # 单样本处理\n",
    "        cost = 0\n",
    "        y_pred = np.empty((1,))\n",
    "        y_true = np.empty((1,))\n",
    "        for i in range(x.shape[0]):\n",
    "            A = forward_propagate(np.expand_dims(x[i], axis=0), W)\n",
    "            cost += loss(A[-1], np.expand_dims(y[i], axis=0))\n",
    "            W = back_propagate(np.expand_dims(x[i], axis=0), W, A, np.expand_dims(y[i], axis=0), config)\n",
    "            y_pred = np.append(y_pred, np.argmax(A[-1],axis=1), axis=0)\n",
    "            y_true = np.append(y_true, np.argmax(np.expand_dims(y[i], axis=0), axis=1), axis=0) \n",
    "\n",
    "        print(\"{} epoch, cost is {}\".format(epoch, cost), sum(list(y_pred[1:]==y_true[1:]))/y.shape[0])\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    config = {\"bias\":False, \"learning_rate\":0.1, \"model_config\":[3, 16, 16, 3], \n",
    "              \"epochs\":200, \"test_size\":0.2, \"batch\":False}\n",
    "    \n",
    "    # 加载数据集\n",
    "    x1 =  [[1.58, 2.32, -5.8], [ 0.67, 1.58, -4.78], [ 1.04, 1.01, -3.63], \n",
    "        [-1.49, 2.18, -3.39], [-0.41, 1.21, -4.73], [1.39, 3.16, 2.87],\n",
    "        [ 1.20, 1.40, -1.89], [-0.92, 1.44, -3.22], [ 0.45, 1.33, -4.38],\n",
    "        [-0.76, 0.84, -1.96]]\n",
    "    x2 = [[ 0.21, 0.03, -2.21], [ 0.37, 0.28, -1.8], [ 0.18, 1.22, 0.16], \n",
    "        [-0.24, 0.93, -1.01], [-1.18, 0.39, -0.39], [0.74, 0.96, -1.16],\n",
    "        [-0.38, 1.94, -0.48], [0.02, 0.72, -0.17], [ 0.44, 1.31, -0.14],\n",
    "        [ 0.46, 1.49, 0.68]]\n",
    "    x3 = [[-1.54, 1.17, 0.64], [5.41, 3.45, -1.33], [ 1.55, 0.99, 2.69], \n",
    "        [1.86, 3.19, 1.51], [1.68, 1.79, -0.87], [3.51, -0.22, -1.39],\n",
    "        [1.40, -0.44, -0.92], [0.44, 0.83, 1.97], [ 0.25, 0.68, -0.99],\n",
    "        [ 0.66, -0.45, 0.08]]\n",
    "    \n",
    "    x = np.vstack((np.array(x1), np.array(x2), np.array(x3)))\n",
    "    y=[0]*len(x1) + [1]*len(x2) + [2]*len(x3)\n",
    "    y = np.eye(3)[y]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=config[\"test_size\"], random_state=66)\n",
    "    \n",
    "    if config['bias']:\n",
    "        x_train = np.c_[x_train, np.ones((x_train.shape[0]))]\n",
    "        \n",
    "    if config[\"batch\"]:\n",
    "        W = bpnn_batch(x_train, y_train, config)\n",
    "    else:\n",
    "        W = bpnn_single(x_train, y_train, config)\n",
    "    print(W)\n",
    "\n",
    "    # 测试效果\n",
    "    if config['bias']:\n",
    "        x_test = np.c_[x_test, np.ones((x_test.shape[0]))]\n",
    "    A = forward_propagate(x_test, W)\n",
    "    cost = loss(A[-1], y_test)\n",
    "    y_pred = np.argmax(A[-1],axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    print(\"Cost:\", cost, \"Accuracy:\", sum(list(y_pred==y_true))/y_test.shape[0])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
